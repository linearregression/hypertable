#summary Instructions on how to get Hypertable up and running with Hadoop

==Starting the Servers==

*Step 1.* Install and start Hadoop.

*Step 2.* Create the directory `/hypertable` in HDFS and make it writeable by all.

{{{
$ hadoop fs -mkdir /hypertable
$ hadoop fs -chmod 777 /hypertable
}}}

*Step 3.* Edit the config file conf/hypertable.cfg  Change the following property to point to the Hadoop filesystem that got up and running in step 1 (assuming hdfs://motherlode000:9000):
{{{
HdfsBroker.fs.default.name=hdfs://motherlode001:9000
}}}
Change the following two properties to point to the location of the Hypertable Master and Hyperspace (assuming motherlode001):
{{{
Hyperspace.Master.Host=motherlode001
Hypertable.Master.Host=motherlode001
}}}

*Step 4.* Configure Capistrano for your specific cluster and KFS.  See [DeployingHypertable How to Deploy Hypertable] for details.  The following is an example of how the variables at the top of the Capfile might be changed for KFS.
{{{
------------- Capfile ----------------
set :source_machine, "motherlode000"
set :install_dir,  "/data1/doug/hypertable" 
set :hypertable_version, "0.9.0.8"
set :dfs, "hadoop"
set :default_config, "/home/doug/conf/cluster1-standard.cfg"

role :master, "motherlode001"
role :slave,  "motherlode001", "motherlode002", "motherlode003", "motherlode004", "motherlode005", "motherlode006", "motherlode007", "motherlode008"
}}}

*Step 5.* Compile the Hypertable code and install under the installation directory (e.g. /data1/doug/hypertable)

*Step 6.* Distribute the installation
{{{
$ cap dist
}}}

*Step 7.* Start the servers
{{{
$ cap start
}}}

Now you sould be able to run the `~/hypertable/bin/hypertable` HQL command interpreter and start playing around.

==Stopping the System==

{{{
$ cap stop
}}}