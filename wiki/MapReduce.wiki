#summary Hadoop MapReduce with Hypertable
<wiki:toc/>

=Hadoop !MapReduce Tutorial=

In this section, we'll walk you through two Hadoop !MapReduce examples, one using Java !MapReduce and the other using Hadoop streaming.  Both examples do the same thing and illustrate how to run !MapReduce jobs that read from and write to a table in Hypertable.

==Setup==

This program assumes there exists a table called _wikipedia_ that has been loaded with a Wikipedia dump.  It reads the _article_ column, tokenizes it, and populates the _word_ column of the same table.  Each unique word in the article turns into a qualified column and the value is the number of times the word appears in the article.

The first step is to get Hadoop up and running.  After that, Hypertable needs to be launched on top of Hadoop, which can be done through Capistrano as described in [#Up_and_Running_with_HDFS], or if just running a local instance of Hadoop, can be launched with:

{{{
/opt/hypertable/current/bin/ht start-all-servers hadoop
}}}

Download and uncompress the Wikipedia dump.  This dump has been converted into `.tsv` format, digestible by LOAD DATA INFILE.

{{{
wget https://s3.amazonaws.com/hypertable-data/wikipedia.tsv.gz
}}}

Create the _wikipedia_ table by executing the following commands in the hypertable shell:

{{{
USE "/";
DROP TABLE IF EXISTS wikipedia;
CREATE TABLE wikipedia (
       title,
       id,
       username,
       article,
       word
);
}}}

==Java !MapReduce Example==

In this example, we'll be running the `WikipediaWordCount.java` program which can be found in the  `examples/java/org/hypertable/examples/hadoop/mapreduce` directory of the source archive.  It is also compiled into the `hadoop-*-examples.jar` file, contained in the binary packages.  The following is a link to the code:

[http://www.hypertable.org/pub/code-examples/WikipediaWordCount.java.txt WikipediaWordCount.java]

Use the following command to load the Wikipedia dump into the _wikipedia_ table:

{{{
USE "/";
load data infile 'wikipedia.tsv.gz' into table wikipedia;
}}}

To get an idea of what the data looks like, try the following select:

{{{
hypertable> select * from wikipedia where row =^ "Addington";
Addington, Buckinghamshire	title	Addington, Buckinghamshire
Addington, Buckinghamshire	id	377201
Addington, Buckinghamshire	username	Roleplayer
Addington, Buckinghamshire	article	{{infobox UK place \n|country = England\n|latitude=51.95095\n|longitude=-0.92177\n|official_name= Addington\n| population = 145 ...
}}}

Now run the !WikipediaWordCount !MapReduce program:

{{{
/opt/hadoop/current/bin/hadoop jar \
    /opt/hypertable/current/lib/java/hypertable-0.9.5.0-examples.jar \  
    org.hypertable.examples.WikipediaWordCount \
    -Dmapred.local.dir="/mnt/hadoop/mapred/local" \
    --columns=article
}}}

To verify that it worked, try selecting for the the _word_column:

{{{
hypertable> select word from wikipedia where row =^ "Addington";
[...]
Addington, Buckinghamshire	word:A	1
Addington, Buckinghamshire	word:Abbey	1
Addington, Buckinghamshire	word:Abbotts	1
Addington, Buckinghamshire	word:According	1
Addington, Buckinghamshire	word:Addington	6
Addington, Buckinghamshire	word:Adstock	1
Addington, Buckinghamshire	word:Aston	1
Addington, Buckinghamshire	word:Aylesbury	3
Addington, Buckinghamshire	word:BUCKINGHAM	1
Addington, Buckinghamshire	word:Bayeux	2
Addington, Buckinghamshire	word:Bene	1
Addington, Buckinghamshire	word:Bishop	1
[...]
}}}

==Hadoop Streaming==

In this example, we'll be running a Hadoop streaming !MapReduce job that uses a Bash script as the mapper and a Bash script as the reducer.  The mapper script (`tokenize-article.sh`) and the reducer script (`reduce-word-counts.sh`) are show below.

*Mapper script (`tokenize-article.sh`)*

{{{
#!/usr/bin/env bash

IFS="	"
read name column article

while [ $? == 0 ] ; do

  if [ "$column" == "article" ] ; then

    # Strip punctuation
    stripped_article=`echo $article | awk 'BEGIN { FS="\t" } { print $NF }' | tr "\!\"#\$&'()*+,-./:;<=>?@[\\\\]^_\{|}~" " " | tr -s " "` ;

    # Split article into words
    echo $stripped_article | awk -v name="$name" 'BEGIN { article=name; FS=" "; } { for (i=1; i<=NF; i++) printf "%s\tword:%s\t1\n", article, $i; }' ;

  fi

  # Read another line
  read name column article

done
exit 0
}}}

*Reducer script (`reduce-word-counts.sh`)*

{{{
#!/usr/bin/env bash

last_article=
last_word=
let total=0

IFS="	"
read article word count

while [ $? == 0 ] ; do
    if [ "$article" == "$last_article" ] && [ "$word" == "$last_word" ] ; then
        let total=$count+total
    else
        if [ "$last_word" != "" ]; then
            echo "$last_article	$last_word	$total"
        fi
        let total=$count
        last_word=$word
        last_article=$article
    fi
    read article word count
done

if [ $total -gt 0 ] ; then
    echo "$last_article	$last_word	$total"
fi
exit 0
}}}

To populate the word column of the wikipedia table by tokenizing the article column using the above mapper and reduce script, issue the following command:

{{{
hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u0.jar \
-libjars /opt/hypertable/current/lib/java/hypertable-0.9.5.0.pre4.jar,/opt/hypertable/current/lib/java/libthrift-0.6.0.jar \
-Dhypertable.mapreduce.input.table=wikipedia -Dhypertable.mapreduce.output.table=wikipedia \
-mapper /home/doug/tokenize-article.sh \
-combiner /home/doug/reduce-word-counts.sh \
-reducer /home/doug/reduce-word-counts.sh \
-file /home/doug/tokenize-article.sh \
-file /home/doug/reduce-word-counts.sh \
-inputformat org.hypertable.hadoop.mapred.TextTableInputFormat \
-outputformat org.hypertable.hadoop.mapred.TextTableOutputFormat \
-input wikipedia -output wikipedia
}}}

===Column Selection===

To run a !MapReduce job over a subset of columns from the input table, specify a comma separated list of columns in the `hypertable.mapreduce.input.scan_spec.columns` Hadoop configuration property.

*Example:*

{{{
hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u0.jar \
-libjars /opt/hypertable/current/lib/java/hypertable-0.9.5.0.pre4.jar,/opt/hypertable/current/lib/java/libthrift-0.6.0.jar \
-Dhypertable.mapreduce.input.table=wikipedia \
-Dhypertable.mapreduce.input.scan_spec.columns="id,title" \
-mapper /bin/cat -reducer /bin/cat \
-inputformat org.hypertable.hadoop.mapred.TextTableInputFormat \
-input wikipedia -output columns
}}}

===Timestamps===

*Timestamp Predicate*.  To filter the input table with a timestamp predicate, specify the timestamp predicate in the `hypertable.mapreduce.input.scan_spec.timestamp_interval` Hadoop configuration property.  The timestamp predicate is specified using the same format as the timestamp predicate in the `WHERE` clause of the `SELECT` statement.  For example:

  * `TIMESTAMP < 2010-08-03 12:30:00`
  * `TIMESTAMP >= 2010-08-03 12:30:00`
  * `2010-08-01 <= TIMESTAMP <= 2010-08-09`

*Preserving Timestamps*.  To preserve the timestamps from the input table, set the `hypertable.mapreduce.input.include_timestamps` Hadoop configuration property to _true_.  This will cause the !TextTableInputFormat class to produce an additional field (field 0) that represents the timestamp as nanoseconds since the epoch.

*Example:*

{{{
hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u0.jar \
-libjars /opt/hypertable/current/lib/java/hypertable-0.9.5.0.pre4.jar,/opt/hypertable/current/lib/java/libthrift-0.6.0.jar \
-Dhypertable.mapreduce.input.table=wikipedia \
-Dhypertable.mapreduce.output.table=wikipedia2 \
-Dhypertable.mapreduce.input.scan_spec.columns="id,title" \
-Dhypertable.mapreduce.input.scan_spec.timestamp_interval="2010-08-01 <= TIMESTAMP <= 2010-08-09" \
-Dhypertable.mapreduce.input.include_timestamps=true \
-mapper /bin/cat -reducer /bin/cat \
-inputformat org.hypertable.hadoop.mapred.TextTableInputFormat \
-outputformat org.hypertable.hadoop.mapred.TextTableOutputFormat \
-input wikipedia -output wikipedia2
}}}
