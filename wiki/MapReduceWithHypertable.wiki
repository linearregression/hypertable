#summary Design,rationale and usage information for the MapReduce support for Hypertable

1. [#Introduction Introduction]

2. [#Design Design information]

 2.1 [#Outline Outline]

 2.2 [#TableInputFormat TableInputFormat]
 
 2.3 [#Context Task context]

= Introduction =

== A word about mapreduce ==
The reader is assumed to have a prior knowledge about the mapreduce algorithm.
The details are located at http://labs.google.com/papers/mapreduce-osdi04.pdf

== Filesystem support for !MapReduce and Hypertable ==

The mapreduce (MR) is currently supported  by the Hadoop DFS only.

= Design =

== Outline ==

The mapreduce _connector_ consist of 2 separate parts:
  * Java classes implementing required interfaces (the requirements stem from Hadoop's MR architecture)
  * C++ Pipes interface

!MapReduce can currently operate on single tables only. It achieves that
in several steps:
  # Creating `TableInputFormat` class
  # Passing task context to the C++ Pipes

=== !TableInputFormat ===

The `TableInputFormat` class implements `InputFormat` interface for describing the data source.It does that by keeping table name and tablet ranges and their corresponding network locations, i.e. ip addresses of servers hosting them. This information is used to construct `TableSplit` objects describing particular tablets which get passed along with the task context information to the map() function.

=== Context ===

Hadoop's mapreduce can operate in 3 different ways:
  # utilizing native Java code
  # using streaming plugin supporting text only data
  # using Pipes API

The current choice is to use Pipes API as it allows to quickly prototype the code
and test the ideas quickly and by no means is a final solution (for more information
please reference *Future Work* section).

The Pipes API receives the task context and an associated `TableSplit` ,created
by the Java counter part of the connector, and starts running. It can accept input from a _pipe_ (hence the name), which works conceptually the same way as the unix "|" (pipe) operator, or by utilizing custom `RecordReader`s. The current choice is to utilize
custom C++ `RecordReader` so that the information contained inside the
tables can be retrieved. Both ways require approach specific configuration.

The _map_ function receives single consecutive cells so it is up to the
person implementing _map_ function to account for that and
handle row boundary mapping. The columns to be passed to the map function
can be configured in the job configuration XML file (please refer to configuration section)

= Compilation =

== Prerequisites ==


= Future Work =

These are things that need to be done in order
to have a fully-featured Hypertable support
  # create `TableOutputFormat` class in Java
  # create C++ `RecordWriter`
  # integrate with thrift broker once it is ready