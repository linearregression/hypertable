#summary Design,rationale and usage information for the MapReduce support for Hypertable

1. [#Introduction Introduction]

2. [#Design Design information]

 2.1 [#Outline Outline]

 2.2 [#TableInputFormat TableInputFormat]
 
 2.3 [#Context Task context]

3. [#Compilation Compiling MapReduce support]

 3.1 [#Prerequisites Prerequisites]
 
 3.2 [#Building_Hadoop Building Hadoop]

 3.3 [#Building_Hypertable Building MapReduce support]

= Introduction =

== A word about mapreduce ==
The reader is assumed to have a prior knowledge about the mapreduce algorithm.
The details are located at http://labs.google.com/papers/mapreduce-osdi04.pdf

== Filesystem support for !MapReduce and Hypertable ==

The mapreduce (MR) is currently supported  by the Hadoop DFS only.

= Design =

== Outline ==

The mapreduce _connector_ consist of 2 separate parts:
  * Java classes implementing required interfaces (the requirements stem from Hadoop's MR architecture)
  * C++ Pipes interface

!MapReduce can currently operate on single tables only. It achieves that
in several steps:
  # Creating `TableInputFormat` class
  # Passing task context to the C++ Pipes

=== !TableInputFormat ===

The `TableInputFormat` class implements `InputFormat` interface for describing the data source.It does that by keeping table name and tablet ranges and their corresponding network locations, i.e. ip addresses of servers hosting them. This information is used to construct `TableSplit` objects describing particular tablets which get passed along with the task context information to the map() function.

=== Context ===

Hadoop's mapreduce can operate in 3 different ways:
  # utilizing native Java code
  # using streaming plugin supporting text only data
  # using Pipes API

The current choice is to use Pipes API as it allows to quickly prototype the code
and test the ideas quickly and by no means is a final solution (for more information
please reference *Future Work* section).

The Pipes API receives the task context and an associated `TableSplit` ,created
by the Java counter part of the connector, and starts running. It can accept input from a _pipe_ (hence the name), which works conceptually the same way as the unix "|" (pipe) operator, or by utilizing custom `RecordReader`s. The current choice is to utilize
custom C++ `RecordReader` so that the information contained inside the
tables can be retrieved. Both ways require approach specific configuration.

The _map_ function receives single consecutive cells so it is up to the
person implementing _map_ function to account for that and
handle row boundary mapping. The columns to be passed to the map function
can be configured in the job configuration XML file (please refer to configuration section)

= Compilation =

== Prerequisites ==

In order to compile MapReduce support for Hypertable you have to download
Java JDK and fulfill usual requirements for the Hypertable.

== Building ==

=== Building Hadoop ===
The first step is to prepare hadoop pipes and hadoop utils libraries. You can do this as follows (note: HADOOP_SRC_DIR is the path to the Hadoop's source code directory)
{{{
$ cd HADOOP_SRC_DIR/src/c++/utils
$ sh configure
$ make && make install  # no need for root, this is local install only
}}}
When compiling hadoopUtils library please make sure you add "-fPIC" code somewhere in the makefile. This is a bug in hadoop. The pipes library has PIC enabled by default.

*WARNING:* compiling with CFLAGS="-fPIC" make won't work as the makefile doesn't append but rather overwrites compile flags.
{{{
$ cd HADOOP_SRC_DIR/src/c++/pipes
$ sh configure
$ make && make install
}}}
Once these libraries are compiled, you have to add their paths to the ld.so.conf file. For example on Ubuntu you have to do the following:
{{{
# echo "$HADOOP_SRC_DIR/lib/native/<your OS>-<your architecture>/" > /etc/ld.so.conf.d/hadoop.conf
}}}
Apply the following patch to the hadoop Pipes source code
{{{
--- src/java/org/apache/hadoop/mapred/pipes/Submitter.java.orig	2008-05-15 09:20:16.000000000 +0200
+++ src/java/org/apache/hadoop/mapred/pipes/Submitter.java	2008-07-23 13:01:58.000000000 +0200
@@ -364,6 +364,14 @@
       }
       if (results.hasOption("-jar")) {
         conf.setJar((String) results.getValue("-jar"));
+        // if they gave us a jar file, include it into the class path
+        String jarFile = conf.getJar();
+        if (jarFile != null) {
+          ClassLoader loader =
+            new URLClassLoader(new URL[]{ FileSystem.getLocal(conf).
+                                          pathToFile(new Path(jarFile)).toURL()});
+          conf.setClassLoader(loader);
+        }
       }
       if (results.hasOption("-inputformat")) {
         setIsJavaRecordReader(conf, true);
@@ -406,14 +414,6 @@
           conf.set(keyValSplit[0], keyValSplit[1]);
         }
       }
-      // if they gave us a jar file, include it into the class path
-      String jarFile = conf.getJar();
-      if (jarFile != null) {
-        ClassLoader loader =
-          new URLClassLoader(new URL[]{ FileSystem.getLocal(conf).
-                                        pathToFile(new Path(jarFile)).toURL()});
-        conf.setClassLoader(loader);
-      }
       submitJob(conf);
     } catch (OptionException oe) {
       cli.printUsage();
}}}

and then compile Hadoop by issuing
{{{
HADOOP_SRC_DIR/$ ant compile
}}}

This will compile the Hadoop with corrected code which will load custom classes
at appropriate moment so that it can read custom _!InputFormat_ implementation. Replace the hadoop-core-\*.jar archive with hadoop-core-\*-dev.jar one. This will replace the old jar file with a new patched code. 

Add the following property to your hadoop-site.xml or hadoop-default.xml (depending on your needs)
{{{
<property>
 <name>hypertable.config.path</name>
 <value>/absolute/path/to/your/hypertable/config</value>
</property>
}}}

Start hadoop as usually.

=== Building Hypertable ===

To build Hypertable with !MapReduce support enabled you have to know the location of your Hadoop source dir (let's call it HADOOP_SRC_DIR) and location of Hadoop's pipes and utils libraries

Let's proceed by
{{{
$ cd HYPERTABLE_SRC_DIR
$ cmake -DBUILD_SHARED_LIBS=ON -DHADOOP_INCLUDE_PATH=$HADOOP_SRC_DIR/src/c++/install/include/hadoop/ -DHADOOP_LIB_PATH=$HADOOP_SRC_DIR/src/c++/install/lib/ .
$ make && sudo make install

}}}
= Future Work =

These are things that need to be done in order
to have a fully-featured Hypertable support
  # create `TableOutputFormat` class in Java
  # create C++ `RecordWriter`
  # integrate with thrift broker once it is ready