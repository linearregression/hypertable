#summary A design for cluster replication
= Introduction =

This document describes a design for cluster replication. With this feature, every update that is sent to a cluster is automatically forwarded asynchronously to a remote cluster. This cluster can be located in a different data center.

= Features =

 * Uni-directional or bi-directional replication
 * Replication is asynchronously (there can be slight delays)
 * Very little administration required
 * Replication is per table, not per column or per cluster
 * Replication can be activated with CREATE TABLE, ALTER TABLE
 * Replication can be deactivated with ALTER TABLE

= Terminology =

 * _Sender_: the replication process on the cluster which is replicated
 * _Receiver_: the replication process on the replica cluster 

= Timeline =

In order to have fast results, we will split the replication feature into two phases:
 # Phase 1
    * A single replication process forwards all updates to the remote cluster
    * A cluster can only be replicated to one other cluster, not to multiple clusters
 # Phase 2
    * A group of replication processes forward all updates to the remote cluster

= User Interface =

When initializing, the Master creates a unique ID for the cluster and stores it in hyperspace, unless the ID was already created.

We will provide a tool or a parameter to overwrite this ID.

We will provide a tool or script to create a mapping of cluster-ID to IP-address/hostname (this information is stored in hyperspace).

The CREATE TABLE statement is modified; it supports a new option.
{{{
    CREATE TABLE t (foo) REPLICATE TO <cluster-id>
}}}
The ALTER TABLE statement is modified:
{{{
    ALTER TABLE t REPLICATE TO <cluster-id> [ON]
    ALTER TABLE t REPLICATE TO <cluster-id> OFF
}}}
It is not allowed to enable replication for index tables or tables in the reserved /sys namespace.

The replication processes (_Sender_ and _Receiver_) have to be started manually. We will provide a startup script.

= Bootstrapping =

When activating replication for an existing table, the existing cells have to be copied manually. This can be done via HQL commands (DUMP TABLE/LOAD DATA). We will document this accordingly.

The tables on the remote cluster have to be created manually.

= Implementation Phase 1 =

Upon initialization, the _Sender_ reads all table schemas from hyperspace to find out which tables need replication. It also fetches the remote addresses of the cluster-IDs from hyperspace.

Afterwards, the _Sender_ loads its existing state from a mml file. The state describes the fragments and file offsets where it stopped when it was running previously.

If there's no such state (i.e. when the _Sender_ was started for the very first time), then it will only start sending updates which are created AFTER the _Sender_ was started. All older updates have to be sent during the bootstrapping process, which was described above.

Periodically, the _Sender_ then scans the commit logs of all replicated tables and proceeds as follows:

{{{
IF the commit log grew at least for a certain buffer size BS: 
    read the new commit log entries
    send them to the Receiver
    repeat reading/sending till the end of the commit log file is reached
    write the Sender state (commit log fragment ID + file offset) to a metalog file
}}}

If the _Sender_ crashes then it will read its current state from the metalog file after it was
restarted. Since the updates are idempotent it is not a problem if parts of a fragment are sent twice. Therefore the updates to the metalog file do not have to be very frequent.

In order to make sure that merged/compacted fragments are not deleted before they are replicated, the RangeServer no longer deletes any fragments in CommitLog::purge. Instead, it renames them to "fragment-id.deleted". The _Sender_ will delete all files with extension ".deleted" after it replicated them (or immediately, if this fragment belongs to a table that is not replicated).

(It would be nice if the RangeServer could immediately delete fragments that are not replicated, instead of renaming them to .deleted.)

When the user enables replication for a new table (either via CREATE TABLE or ALTER TABLE) then the Master informs the _Sender_ that it has to re-read its Schema info from Hyperspace. The _Sender_ will then find out that a new table needs to be replicated, and will start processing the log fragments accordingly.

On the remote cluster, the _Receiver_ will pick up incoming updates from the _Sender_ and append them to a queue in the DFS. Then it uses a regular TableMutator to write them. If the _Receiver_ (or any node in the cluster) fails or crashes, then all updates which have not yet been processed  are still available in the log file. 

When the _Receiver_ starts, it therefore first processes this queue, and afterwards it continues with the regular updates.

If the _Receiver_ crashes, the _Sender_ will just stop sending data till it the _Receiver_ is running again. In case the _Sender_ is unavailable, it will use the notification-hook to send an email to the Administrator.


= Implementation Phase 2 =

TBD
  * needs capistrano support for multiple _Senders_
  * needs a central master that controls multiple _Senders_
  * ...?

= Open Questions =

- How to enable bi-directional updates without having the updates "run in circles"? We would have to store the IDs (or checksums of those IDs) of all clusters which already received this update. These IDs would have to be stored in the CellStore with every single update. Is there an easier solution?